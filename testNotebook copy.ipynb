{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f1eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8d1feec07d487d999e1d37ca945729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import textwrap\n",
    "from transformers.utils import logging as hf_logging\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "hf_logging.set_verbosity_error() \n",
    "\n",
    "whichModel = \"3B\"  # Options are Llama-3.2-1B-Instruct (1B) | Llama-3.2-3B-Instruct (3B)\n",
    "pathToModel = f\"/Users/raymo/Desktop/is389/Models/Llama-3.2-{whichModel}-Instruct\"  # fString for filepath concat\n",
    "tokenizer = AutoTokenizer.from_pretrained(pathToModel)  # initialize tokenizer\n",
    "pipe = pipeline(\"text-generation\", model=pathToModel, tokenizer=tokenizer, dtype=torch.bfloat16, return_full_text=False, device_map=\"auto\")\n",
    "\n",
    "# Prints neatly\n",
    "def prettyprint(dirtyprint, \n",
    "       width=100):\n",
    "    wrappedText = textwrap.fill(dirtyprint, width=width)\n",
    "    print(wrappedText)\n",
    "    print()\n",
    "    return\n",
    "\n",
    "# Finds Length (in tokens) Of Context\n",
    "\n",
    "def getPromptLength(msgs, tokenizer):\n",
    "    ids = tokenizer.apply_chat_template(\n",
    "        msgs, add_generation_prompt=True, tokenize=True\n",
    "    )\n",
    "    # ids can be: list[int], list[list[int]], or a tensor\n",
    "    try:\n",
    "        # tensor-like (has shape)\n",
    "        shape = ids.shape  # raises if not tensor\n",
    "        return ids.shape[-1]\n",
    "    except AttributeError:\n",
    "        # python list path\n",
    "        if ids and isinstance(ids[0], list):\n",
    "            ids = ids[0]\n",
    "        return len(ids)\n",
    "\n",
    "# trim oldest chat turns until prompt fits\n",
    "def knife(messages, tokenizer, limit, reserve=64):\n",
    "    if not messages:\n",
    "        return messages\n",
    "\n",
    "    keepStart = 1 if messages and messages[0].get(\"role\") == \"system\" else 0\n",
    "    target = max(limit - reserve, 0)\n",
    "\n",
    "    while getPromptLength(messages, tokenizer) > target and len(messages) > (keepStart + 1):\n",
    "        # remove the oldest non-system message\n",
    "        messages.pop(keepStart)\n",
    "    return messages\n",
    "\n",
    "# Gets response from LLM\n",
    "def getModelResponse(messages, pipe, tokenizer, max_new_tokens=256, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "\n",
    "    # Build model-ready prompt using chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Stop on either <|eot_id|> (EOT) or </s> (EOS)\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=[eot_id, eos_id],\n",
    "        return_full_text=False,  # only the assistant text\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc8889a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chef(systemPrompt, pipe, tokenizer, messages=None, turn=0,\n",
    "         maxContextTokens=4096, max_new_tokens=256):\n",
    "\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    # ensure system message at front always\n",
    "    if not messages or messages[0].get(\"role\") != \"system\":\n",
    "        messages = [{\"role\": \"system\", \"content\": systemPrompt}, *messages]\n",
    "\n",
    "    userPrompt = input(\"Ask anything. qq to quit.\")\n",
    "    prettyprint(f\"[{turn}] [USER] {userPrompt}\")\n",
    "\n",
    "    if userPrompt.strip().lower() == \"qq\":\n",
    "        prettyprint(\"[NOTICE] CONVERSATION ENDED WITH QQ\")\n",
    "        return\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": userPrompt})\n",
    "\n",
    "    # trims context and leaves room for next assistant response and stop tokens\n",
    "    reserve_for_generation = max_new_tokens + 8\n",
    "    messages = knife(messages, tokenizer, limit=maxContextTokens, reserve=reserve_for_generation)\n",
    "\n",
    "    # build prompt using chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    # prettyprint(f\"[{turn}] [CHAT TEMPLATE PROMPT] {prompt}\")\n",
    "\n",
    "    # generate with proper stop tokens\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=[eot_id, eos_id],\n",
    "        return_full_text=False,  # only the assistant's reply\n",
    "    )\n",
    "\n",
    "    response = outputs[0][\"generated_text\"].strip()\n",
    "    prettyprint(f\"[{turn}] [RESPONSE] {response}\")\n",
    "\n",
    "    # add assistant turn\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    # print(messages)\n",
    "\n",
    "    # recursive continuation of chat\n",
    "    return chef(systemPrompt, pipe, tokenizer, messages=messages,\n",
    "                turn=turn+1, maxContextTokens=maxContextTokens, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "073ea78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newfriend = \"You are a person. You are meeting someone new for the first time. Recall facts that the user has provided while responding with realistic responses with respect to meeting someone new for the first time.\"\n",
    "# chef(systemPrompt=newfriend, pipe=pipe, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d76158fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplarsCOT = [\n",
    "    {\"input\":\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\", \n",
    "     \"cot\":\"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11.\",\n",
    "     \"answer\":\"11\"}, \n",
    "\n",
    "    {\"input\":\"Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\", \n",
    "     \"cot\":\"There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392.\",\n",
    "     \"answer\":\"(b)\"}, \n",
    "\n",
    "    {\"input\":\"Q: Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock\", \n",
    "     \"cot\":\"The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do.\",\n",
    "     \"answer\":\"(b)\"},\n",
    "     \n",
    "    {\"input\":\"Q: Yes or no: Would a pear sink in water?\", \n",
    "     \"cot\":\"The density of a pear is about 0.6g/cm^3, which is less than that of water. Thus a pear would float.\",\n",
    "     \"answer\":\"No\"},\n",
    "\n",
    "    {\"input\":\"Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day, and is now today. What is the date 10 days ago in MM/DD/YYYY?\", \n",
    "     \"cot\":\"One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943.\",\n",
    "     \"answer\":\"05/23/1943\"},\n",
    "\n",
    "    {\"input\":\"Q: Is the following sentence plausible? 'Joao Moutinho caught the screen pass in the NFC championship.'\", \n",
    "     \"cot\":\"Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer.\",\n",
    "     \"answer\":\"No\"},\n",
    "\n",
    "    {\"input\":\"Q: Take the last letters of the words in 'Lady Gaga' and concatenate them.\", \n",
    "     \"cot\":\"The last letter of 'Lady' is 'y'. The last letter of 'Gaga' is 'a'. Concatenating them is 'ya'.\",\n",
    "     \"answer\":\"ya\"},\n",
    "\n",
    "    {\"input\":\"Q: A coin is heads up. Maybelle flips the coin over. Shalonda does not flip the coin. Is the coin still heads up?\", \n",
    "     \"cot\":\"The coin was flipped over by Maybelle, so the coin was flipped over 1 time, which is an odd number of times. The coin started heads up, so after an odd number of flips, it will be the opposite of that, which is tails up.\",\n",
    "     \"answer\":\"No\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f4e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a person. You are meeting someone new for the first time. Recall facts that the user has provided while responding with realistic responses with respect to meeting someone new for the first time.Use a private scratchpad for reasoning, then output ONLY the final answer. Format strictly as: <scratchpad>hidden notes</scratchpad><final>concise final answer</final>. Do not reveal or summarize the scratchpad. If you cannot follow the format, output only <final>.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addCOT(baseSystemPrompt):\n",
    "    return baseSystemPrompt + \"Use a private scratchpad for reasoning, then output ONLY the final answer. Format strictly as: <scratchpad>hidden notes</scratchpad><final>concise final answer</final>. Do not reveal or summarize the scratchpad. If you cannot follow the format, output only <final>.\"\n",
    "\n",
    "addCOT(newfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8915dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertExemplarsToChatTemplate(exemplars, baseSystemPrompt):\n",
    "    messages = [{\"role\":\"system\",\n",
    "                 \"content\":addCOT(baseSystemPrompt)}]\n",
    "\n",
    "    for i in exemplars:\n",
    "        messages.append({\"role\":\"user\",\n",
    "                         \"content\":i[\"input\"]})\n",
    "        messages.append({\"role\":\"assistant\", \"content\":f\"<scratchpad>{i['cot']}</scratchpad><final>{i['answer']}</final>\"})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46b48b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reFinal = re.compile(r\"<final>(.*?)</final>\", flags=re.S | re.I)\n",
    "reScratch = re.compile(r\"<scratchpad>(.*?)</scratchpad>\", flags=re.S | re.I)\n",
    "\n",
    "def extractFinal(text):\n",
    "    m = reFinal.search(text)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "def extractScratch(text):\n",
    "    m = reScratch.search(text)\n",
    "    return m.group(1).strip() if m else \"\"  # empty if model forgot the tag\n",
    "\n",
    "# Return [scratchpad, final]\n",
    "def extractBoth(text):\n",
    "    return extractScratch(text), extractFinal(text)\n",
    "\n",
    "def cotGenerateOnce(messages, pipe, tokenizer, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    out = pipe(prompt,max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, eos_token_id=[eot_id, eos_id], return_full_text=False)\n",
    "    text = out[0][\"generated_text\"]\n",
    "    return extractBoth(text)\n",
    "\n",
    "def cotAnswer(messages, pipe, tokenizer, k, **gen_kwargs):\n",
    "    pairs = [cotGenerateOnce(messages, pipe, tokenizer, **gen_kwargs) for _ in range(k)]\n",
    "\n",
    "    # pairs [(scratchpad, final)]\n",
    "    finals = [f for (_s, f) in pairs]\n",
    "    bestFinal = Counter(finals).most_common(1)[0][0]\n",
    "\n",
    "    # pick a representative scratchpad for the winning final (first match)\n",
    "    represantativeScratch = next(s for (s, f) in pairs if f == bestFinal)\n",
    "    return bestFinal, pairs, represantativeScratch\n",
    "\n",
    "# Build once at startup\n",
    "exampleMessages = convertExemplarsToChatTemplate(exemplarsCOT, baseSystemPrompt=newfriend)\n",
    "\n",
    "def askAIWithCOT(query, pipe, tokenizer, k, show_cot=False):\n",
    "    prettyprint(query)\n",
    "    msgs = exampleMessages + [{\"role\": \"user\", \"content\": query}]\n",
    "    bestFinal, pairs, represantativeScratch = cotAnswer(msgs, pipe, tokenizer, k=k, max_new_tokens=256, temperature=0.7, top_p=0.9)\n",
    "    if show_cot:\n",
    "        # Print all samples' scratchpads and finals\n",
    "        for i, (scr, fin) in enumerate(pairs, 1):\n",
    "            prettyprint(f\"\\nSample {i}\")\n",
    "            prettyprint(f\"<scratchpad>\\n{scr}\\n</scratchpad>\")\n",
    "            prettyprint(f\"<final> {fin} </final>\")\n",
    "        prettyprint(\"\\nMajority Vote\")\n",
    "        prettyprint(f\"<final> {bestFinal} </final>\")\n",
    "        prettyprint(\"\\nRepresentative COT for the winning answer:\")\n",
    "        prettyprint(f\"<scratchpad>\\n{represantativeScratch}\\n</scratchpad>\")\n",
    "    return bestFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b1ddb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: If John has 50 apples, how many apples does he have if Adam has 10 apples, and John buys 200\n",
      "apples from Derek?\n",
      "\n",
      " Sample 1\n",
      "\n",
      "<scratchpad> John starts with 50 apples. He buys 200 more apples. 50 + 200 = 250. </scratchpad>\n",
      "\n",
      "<final> 250 </final>\n",
      "\n",
      " Majority Vote\n",
      "\n",
      "<final> 250 </final>\n",
      "\n",
      " Representative COT for the winning answer:\n",
      "\n",
      "<scratchpad> John starts with 50 apples. He buys 200 more apples. 50 + 200 = 250. </scratchpad>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'250'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleQuestion = \"If John has 50 apples, how many apples does he have if Adam has 10 apples, and John buys 200 apples from Derek?\"\n",
    "askAIWithCOT(f\"Q: {sampleQuestion}\", pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1\n",
      "<scratchpad>\n",
      "The total cost of the sedan is the initial amount plus the additional amount needed, so 7000 + 3000 = 10000 dollars.\n",
      "</scratchpad>\n",
      "<final> 10000 </final>\n",
      "\n",
      "Sample 2\n",
      "<scratchpad>\n",
      "The difference between 7000 and 3000 is 4000. The sedan costs 4000 dollars.\n",
      "</scratchpad>\n",
      "<final> 4000 </final>\n",
      "\n",
      "Sample 3\n",
      "<scratchpad>\n",
      "The amount needed is 3000. The amount I have is 7000. The difference is 3000. The sedan costs 3000.\n",
      "</scratchpad>\n",
      "<final> 3000 </final>\n",
      "\n",
      "Sample 4\n",
      "<scratchpad>\n",
      "7000 + 3000 = 10000. The sedan costs 10000 dollars.\n",
      "</scratchpad>\n",
      "<final> 10000 </final>\n",
      "\n",
      "Sample 5\n",
      "<scratchpad>\n",
      "7000 + 3000 = 10000 dollars. The sedan costs 10000 dollars.\n",
      "</scratchpad>\n",
      "<final> 10000 </final>\n",
      "\n",
      "Sample 6\n",
      "<scratchpad>\n",
      "The amount needed is 3000 dollars. The amount of money I have is 7000 dollars. So the amount of money the sedan costs is 7000 - 3000 = 4000 dollars.\n",
      "</scratchpad>\n",
      "<final> 4000 </final>\n",
      "\n",
      "Sample 7\n",
      "<scratchpad>\n",
      "The difference between 7000 and 3000 is 4000. That is the cost of the sedan.\n",
      "</scratchpad>\n",
      "<final> 4000 </final>\n",
      "\n",
      "Majority Vote\n",
      "<final> 10000 </final>\n",
      "\n",
      "Representative COT for the winning answer:\n",
      "<scratchpad>\n",
      "The total cost of the sedan is the initial amount plus the additional amount needed, so 7000 + 3000 = 10000 dollars.\n",
      "</scratchpad>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10000'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askAIWithCOT(f\"Q:{input()}\", pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9281d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"A bat and a ball cost £1.10 in total. The bat costs £1.00 more than the ball. How much does the ball cost?\",\n",
    "             \"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\",\n",
    "             \"In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half of the lake?\",\n",
    "             \"If John can drink one barrel of water in 6 days, and Mary can drink one barrel of water in 12 days, how long would it take them to drink one barrel of water together?\",\n",
    "             \"Jerry received both the 15th highest and the 15th lowest mark in the class. How many students are in the class?\",\n",
    "             \"A man buys a pig for £60, sells it for £70, buys it back for £80, and sells it finally for £90. How much has he made?\",\n",
    "             \"Simon decided to invest £8,000 in the stock market one day early in 2008.  Six months after he invested, on July 17, the stocks he had purchased were down 50%. Fortunately for Simon, from July 17 to October 17, the stocks he had purchased went up 75%. At this point, has Simon lost money or gained money?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f8d3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bat and a ball cost £1.10 in total. The bat costs £1.00 more than the ball. How much does the ball\n",
      "cost?\n",
      "\n",
      " Sample 1\n",
      "\n",
      "<scratchpad> The bat costs £1.00 more than the ball. Let's call the cost of the ball x. The cost of\n",
      "the bat is x + £1.00. The total cost is £1.10. So, x + (x + £1.00) = £1.10. 2x + £1.00 = £1.10. 2x =\n",
      "£0.10. x = £0.05. </scratchpad>\n",
      "\n",
      "<final> £0.05 </final>\n",
      "\n",
      " Majority Vote\n",
      "\n",
      "<final> £0.05 </final>\n",
      "\n",
      " Representative COT for the winning answer:\n",
      "\n",
      "<scratchpad> The bat costs £1.00 more than the ball. Let's call the cost of the ball x. The cost of\n",
      "the bat is x + £1.00. The total cost is £1.10. So, x + (x + £1.00) = £1.10. 2x + £1.00 = £1.10. 2x =\n",
      "£0.10. x = £0.05. </scratchpad>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'£0.05'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askAIWithCOT(questions[0], pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ccb108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100\n",
      "widgets?\n",
      "\n",
      " Sample 1\n",
      "\n",
      "<scratchpad> It takes 5 machines 5 minutes to make 5 widgets. This means it takes 1 machine 5\n",
      "minutes to make 1 widget. To make 100 widgets, it would take 1 machine 100 minutes, or 1 hour and 40\n",
      "minutes. 100 machines would make 100 widgets in 5 minutes. </scratchpad>\n",
      "\n",
      "<final> 5 minutes </final>\n",
      "\n",
      " Majority Vote\n",
      "\n",
      "<final> 5 minutes </final>\n",
      "\n",
      " Representative COT for the winning answer:\n",
      "\n",
      "<scratchpad> It takes 5 machines 5 minutes to make 5 widgets. This means it takes 1 machine 5\n",
      "minutes to make 1 widget. To make 100 widgets, it would take 1 machine 100 minutes, or 1 hour and 40\n",
      "minutes. 100 machines would make 100 widgets in 5 minutes. </scratchpad>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5 minutes'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askAIWithCOT(questions[1], pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fa96841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days\n",
      "for the patch to cover the entire lake, how long would it take for the patch to cover half of the\n",
      "lake?\n",
      "\n",
      " Sample 1\n",
      "\n",
      "<scratchpad> The patch doubles in size every day, which means it's growing exponentially. If it\n",
      "covers the entire lake in 48 days, it must have covered half of the lake the day before, or 47 days\n",
      "ago. </scratchpad>\n",
      "\n",
      "<final> 47 </final>\n",
      "\n",
      " Majority Vote\n",
      "\n",
      "<final> 47 </final>\n",
      "\n",
      " Representative COT for the winning answer:\n",
      "\n",
      "<scratchpad> The patch doubles in size every day, which means it's growing exponentially. If it\n",
      "covers the entire lake in 48 days, it must have covered half of the lake the day before, or 47 days\n",
      "ago. </scratchpad>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'47'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askAIWithCOT(questions[2], pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If John can drink one barrel of water in 6 days, and Mary can drink one barrel of water in 12 days,\n",
      "how long would it take them to drink one barrel of water together?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "askAIWithCOT(questions[3], pipe, tokenizer, k=1, show_cot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8940585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
